{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 232,
   "execution_count": 14,
>>>>>>> 8b083bea1c6b9bf6be5ce30e78a4273dc8d47544
   "metadata": {},
   "outputs": [],
   "source": [
    "#from twitter_preprocess import *\n",
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigram counts for data\n",
    "def get_unigrams_splitBySpace():\n",
    "    unigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = get_unigrams_splitBySpace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigram counts for data\n",
    "def get_unigrams_nltkTokenizer():\n",
    "    uni = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            uni[word] += 1  \n",
    "    return uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n-gram, note here n = 5\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(n):\n",
    "    n_grams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        tokens = [token for token in s if token != \"\"]\n",
    "        output = list(ngrams(tokens, n))\n",
    "        n_grams = Counter(output)\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_bigrams():\n",
    "    bigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        word = start + \" \" + s[0]\n",
    "        bigrams[word] += 1\n",
    "        for i in range(len(s)-1):\n",
    "            word = s[i] + \" \" + s[i+1]\n",
    "            bigrams[word] += 1\n",
    "        word = s[len(s) - 1] + \" \" + end\n",
    "        bigrams[word] += 1\n",
    "    #print(bigrams)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gets the average word counts for data\n",
    "#maybe split tweet on more than just whitespace ie ;:,.')(\n",
    "def get_avg_wc():\n",
    "    wcs = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot = 0.\n",
    "        for word in s:\n",
    "            if \"http://\" in word: continue #ignore hyperlinks\n",
    "            tot += len(word)\n",
    "        wcs[' '.join(s)] = tot / len(s)\n",
    "    return wcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the count of '@'s in the data\n",
    "def get_at_counts():\n",
    "    ats = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        ats[' '.join(s)] = count\n",
    "            \n",
    "    return ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the count of swear words in tweets\n",
    "#potentially use regex's to catch purposeful mispellings & other nuances\n",
    "def get_swear_counts():\n",
    "    tweets = train_data[['tweet']]\n",
    "#     bad_word_list = open(\"en_bad_words.txt\").read().replace(\"\\n\", \"\").split(',')\n",
    "#     for i in range(len(bad_word_list)):\n",
    "#         bad_word_list[i] = bad_word_list[i].strip(\" \")\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "    \n",
    "    bad_words_count = {}\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            bad_words_count[\" \".join(s)] = tot_bad\n",
    "            \n",
    "    return bad_words_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram_counts = get_unigrams_splitBySpace()\n",
    "unis = get_ngrams(1)\n",
    "bigrams = get_ngrams(2)\n",
    "trigrams = get_ngrams(3)\n",
    "#uni_tokenizer_counts = get_unigrams_nltkTokenizer()\n",
    "#bigram_counts = get_bigrams()\n",
    "#avg_wc = get_avg_wc()\n",
    "#at_counts = get_at_counts()\n",
    "#swear_counts = get_swear_counts()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 17,
>>>>>>> 8b083bea1c6b9bf6be5ce30e78a4273dc8d47544
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12238    1\n",
       "11389    1\n",
       "14445    1\n",
       "21900    0\n",
       "7795     1\n",
       "20770    1\n",
       "5922     2\n",
       "13962    1\n",
       "24349    1\n",
       "24256    1\n",
       "14680    1\n",
       "915      2\n",
       "4141     1\n",
       "11214    1\n",
       "16702    1\n",
       "18845    2\n",
       "23088    2\n",
       "14684    1\n",
       "5453     1\n",
       "12222    1\n",
       "950      2\n",
       "19294    1\n",
       "4231     0\n",
       "16774    1\n",
       "14573    2\n",
       "490      1\n",
       "1410     1\n",
       "9266     1\n",
       "12664    1\n",
       "13142    1\n",
       "        ..\n",
       "24065    1\n",
       "1099     1\n",
       "11158    1\n",
       "12871    1\n",
       "15147    1\n",
       "23572    1\n",
       "14353    1\n",
       "14071    1\n",
       "16435    1\n",
       "899      1\n",
       "6177     2\n",
       "4235     1\n",
       "22468    1\n",
       "13860    1\n",
       "11068    1\n",
       "4522     1\n",
       "14647    1\n",
       "21714    1\n",
       "5985     1\n",
       "6608     1\n",
       "7303     1\n",
       "20007    1\n",
       "14475    1\n",
       "11895    1\n",
       "8943     2\n",
       "14286    1\n",
       "22275    1\n",
       "13094    2\n",
       "10736    1\n",
       "3388     1\n",
       "Name: class, Length: 22214, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top 100 unigrams and bigrams\n",
<<<<<<< HEAD
    "top_unigrams = unis.most_common(100)\n",
    "top_bigrams = bigrams.most_common(100)\n",
    "top_trigrams = trigrams.most_common(100)\n",
    "#top_unigrams = unigram_counts.most_common(100)\n",
    "#top_tokenized = uni_tokenizer_counts.most_common(100)\n",
    "#top_bigrams = bigram_counts.most_common(100)\n",
    "data = pd.concat([train_data, dev_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ngram_tweets(tweets, model):\n",
    "    for word in [u[0] for u in model]:\n",
    "        if len(word) > 1:\n",
    "            tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        tweet_words = tweet.split()\n",
    "        word_counts.append(len(tweet))\n",
    "        tot_bad = 0\n",
    "        for word in tweet:            \n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        swear_counts.append(tot_bad)\n",
    "        at_count = tweet.count('@')\n",
    "        at_counts.append(at_count)\n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    return tweets[[col for col in tweets.columns if col!=\"tweet\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "tweets = data[['tweet']]\n",
    "X_uni = process_ngram_tweets(tweets, \"top_unigrams\")\n",
    "X_bi = process_ngram_tweets(tweets, \"top_bigrams\")\n",
    "X_tri = process_ngram_tweets(tweets, \"top_trigrams\")\n",
    "y = data['class'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model):\n",
    "    sample_size = len(model)\n",
    "    if sample_size > 10:\n",
    "        cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "    else: \n",
    "        cv = KFold(n_splits=3, random_state=42, shuffle=False)\n",
    "    LR_scores = []\n",
    "    SVM_scores = []\n",
    "\n",
    "    for train_index, test_index in cv.split(model):\n",
    "        LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "        svm = SVC(gamma='auto') \n",
    "        X_train, X_test, y_train, y_test = model[train_index], model[test_index], y[train_index], y[test_index]\n",
    "        LR.fit(X_train, y_train)\n",
    "        LR_scores.append(LR.score(X_test, y_test))\n",
    "        svm.fit(X_train, y_train)\n",
    "        SVM_scores.append(svm.score(X_test, y_test))\n",
    "    return LR_scores, SVM_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_uni, SVM_uni = eval_model(X_uni)\n",
    "LR_bi, SVM_bi =eval_model(X_bi)\n",
    "LR_tri, SVM_tri =eval_model(X_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7628262826282628,\n",
       " 0.7677767776777678,\n",
       " 0.7781278127812782,\n",
       " 0.7812781278127813,\n",
       " 0.7712742008104457,\n",
       " 0.7573165240882486,\n",
       " 0.7730751913552454,\n",
       " 0.7807294011706438,\n",
       " 0.7726249437190454,\n",
       " 0.7879333633498424]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_uni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7637263726372637,\n",
       " 0.77002700270027,\n",
       " 0.7772277227722773,\n",
       " 0.7817281728172817,\n",
       " 0.7726249437190454,\n",
       " 0.7573165240882486,\n",
       " 0.7730751913552454,\n",
       " 0.7816298964430437,\n",
       " 0.7739756866276452,\n",
       " 0.7888338586222422]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_uni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7628262826282628,\n",
       "  0.7677767776777678,\n",
       "  0.7781278127812782,\n",
       "  0.7812781278127813,\n",
       "  0.7712742008104457,\n",
       "  0.7573165240882486,\n",
       "  0.7730751913552454,\n",
       "  0.7807294011706438,\n",
       "  0.7726249437190454,\n",
       "  0.7879333633498424],\n",
       " [0.7637263726372637,\n",
       "  0.77002700270027,\n",
       "  0.7772277227722773,\n",
       "  0.7817281728172817,\n",
       "  0.7726249437190454,\n",
       "  0.7573165240882486,\n",
       "  0.7730751913552454,\n",
       "  0.7816298964430437,\n",
       "  0.7739756866276452,\n",
       "  0.7888338586222422])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_bi, SVM_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7628262826282628,\n",
       "  0.7677767776777678,\n",
       "  0.7781278127812782,\n",
       "  0.7812781278127813,\n",
       "  0.7712742008104457,\n",
       "  0.7573165240882486,\n",
       "  0.7730751913552454,\n",
       "  0.7807294011706438,\n",
       "  0.7726249437190454,\n",
       "  0.7879333633498424],\n",
       " [0.7637263726372637,\n",
       "  0.77002700270027,\n",
       "  0.7772277227722773,\n",
       "  0.7817281728172817,\n",
       "  0.7726249437190454,\n",
       "  0.7573165240882486,\n",
       "  0.7730751913552454,\n",
       "  0.7816298964430437,\n",
       "  0.7739756866276452,\n",
       "  0.7888338586222422])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_tri, SVM_tri"
=======
    "top_unigrams = unigram_counts.most_common(100)\n",
    "top_tokenized = uni_tokenizer_counts.most_common(100)\n",
    "data = pd.concat([train_data, dev_data])\n",
    "top_bigrams = bigram_counts.most_common(100)\n",
    "data['class']"
>>>>>>> 8b083bea1c6b9bf6be5ce30e78a4273dc8d47544
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    for word in [u[0] for u in top_unigrams]:\n",
    "        tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    #print(tweets)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        tweet_words = tweet.split()\n",
    "        word_counts.append(len(tweet_words))\n",
    "        tot_bad = 0\n",
    "        #print(tweet_words)\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        swear_counts.append(tot_bad)\n",
    "        at_count = tweet_words.count('@')\n",
    "        at_counts.append(at_count)\n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokenized_tweets(tweets):\n",
    "    for word in [u[0] for u in top_tokenized]:\n",
    "        if len(word) > 1:\n",
    "            tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        tweet_words = tweet.split()\n",
    "        word_counts.append(len(tweet))\n",
    "        tot_bad = 0\n",
    "        for word in tweet:                #Use regexs? \n",
    "            #word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        swear_counts.append(tot_bad)\n",
    "        at_count = tweet.count('@')\n",
    "        at_counts.append(at_count)\n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    X_tokenized = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_tweets(tweets)\n",
    "X_tokenized = process_tokenized_tweets(tweets)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       [1, 0, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,

   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda2/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda2/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda2/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda2/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],

   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "LR_scores = []\n",
    "SVM_scores = []\n",
    "NB_scores = []\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "    svm = SVC(gamma='auto') \n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    LR.fit(X_train, y_train)\n",
    "    y_LR_predict = LR.predict(X_test)\n",
    "    LR_scores.append(precision_recall_fscore_support(y_test, y_LR_predict, average='macro', labels=[0, 1, 2]))\n",
    "                     \n",
    "    svm.fit(X_train, y_train)\n",
    "    y_svm_predict = svm.predict(X_test)\n",
    "    SVM_scores.append(precision_recall_fscore_support(y_test, y_svm_predict, average='macro', labels=[0, 1, 2]))\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_NB_predict = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    NB_scores.append(precision_recall_fscore_support(y_test, y_NB_predict, average='macro', labels=[0, 1, 2]))\n",
    "#naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_precision(results):\n",
    "    total = 0\n",
    "    num_rows = 0\n",
    "    for row in results:\n",
    "        total += row[0]\n",
    "        num_rows += 1\n",
    "    return total / num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "LR_scores_tok = []\n",
    "SVM_scores_tok = []\n",
    "\n",
    "for train_index, test_index in cv.split(X_tokenized):\n",
    "    LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "    svm = SVC(gamma='auto') \n",
    "    X_train, X_test, y_train, y_test = X_tokenized[train_index], X_tokenized[test_index], y[train_index], y[test_index]\n",
    "    LR.fit(X_train, y_train)\n",
    "    LR_scores_tok.append(LR.score(X_test, y_test))\n",
    "    svm.fit(X_train, y_train)\n",
    "    SVM_scores_tok.append(svm.score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8361836183618362,\n",
       " 0.8352835283528353,\n",
       " 0.851935193519352,\n",
       " 0.873987398739874,\n",
       " 0.8536695182350292,\n",
       " 0.8631247185952273,\n",
       " 0.8572714993246285,\n",
       " 0.8712291760468257,\n",
       " 0.8635749662314273,\n",
       " 0.8703286807744259]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_scores_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8618361836183618,\n",
       " 0.8550855085508551,\n",
       " 0.8672367236723673,\n",
       " 0.8861386138613861,\n",
       " 0.8716794236830256,\n",
       " 0.875281404772625,\n",
       " 0.8779828905898244,\n",
       " 0.8928410625844214,\n",
       " 0.8829356145880234,\n",
       " 0.8793336334984241]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [

    "LR_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8186318631863186,\n",
       " 0.8181818181818182,\n",
       " 0.833033303330333,\n",
       " 0.8424842484248425,\n",
       " 0.819900945520036,\n",
       " 0.8212516884286357,\n",
       " 0.829806393516434,\n",
       " 0.845114813147231,\n",
       " 0.820351193156236,\n",
       " 0.8460153084196308]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_scores_tok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8604860486048604,\n",
       " 0.8577857785778578,\n",
       " 0.864986498649865,\n",
       " 0.8771377137713772,\n",
       " 0.8680774425934263,\n",
       " 0.8671769473210266,\n",
       " 0.8766321476812247,\n",
       " 0.884736605132823,\n",
       " 0.8757316524088249,\n",
       " 0.8833858622242233]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_scores"

    "LR_precision = get_avg_precision(LR_scores)\n",
    "SVM_precision = get_avg_precision(SVM_scores)\n",
    "NB_precision = get_avg_precision(NB_scores)\n",
    "LR_precision, SVM_precision, NB_precision"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
