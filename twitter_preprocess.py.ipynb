{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TWITTER_USERNAME = r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z0-9_]+)'\n",
    "# TWITTER_URL = r'http:\\/\\/t\\.co\\/[A-Za-z0-9_]+'\n",
    "\n",
    "def setup_args():\n",
    "  parser = argparse.ArgumentParser(description='Preprocess the data.')\n",
    "  parser.add_argument('--twitter', dest='twitter', action='store_const',\n",
    "                     const=True, default=False,\n",
    "                     help='This is Twitter data')\n",
    "  parser.add_argument('--rewrite', dest='rewrite', action='store_const',\n",
    "                     const=True, default=False,\n",
    "                     help='Clean and write data again')\n",
    "  parser.add_argument('datafile', metavar='df', help='The data file to clean')\n",
    "\n",
    "  return parser.parse_args()\n",
    "\n",
    "def split_hashtag(hashtag):\n",
    "  hashtag_body = hashtag[1:]\n",
    "  if hashtag_body.upper() == hashtag_body:\n",
    "    result = '<hashtag> ' + hashtag_body + ' <allcaps>'\n",
    "  else:\n",
    "    result = '<hashtag> ' + ' '.join(hashtag_body.split('(?=[A-Z])'))\n",
    "  return result\n",
    "\n",
    "def clean(text, twitter=True):\n",
    "  text = re.sub(r'&#8220;|&#8221;|\"', '', text)\n",
    "  text = re.sub(r'&#[0-9]{3,8};', ' <sym> ', text)\n",
    "  text = re.sub(r'&gt;', '>', text)\n",
    "  text = re.sub(r'&lt;', '<', text)\n",
    "\n",
    "  if twitter:\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    smile = eyes + nose + '[)dp]+|[(dp]+' + nose + eyes\n",
    "    sadface = eyes + nose + '\\(+|\\)+' + nose + eyes\n",
    "    neutralface = eyes + nose + '[\\/|l*]'\n",
    "\n",
    "    text = re.sub(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*',\" <url> \", text)\n",
    "    text = re.sub(\"/\",\" / \", text) # Force splitting words appended with slashes (once we tokenized the URLs, of course)\n",
    "    text = re.sub(r'@\\w+', \" <user> \", text)\n",
    "    text = re.sub(smile+ '|^[._]^', \"<smile>\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(sadface, \"<sadface>\", text)\n",
    "    text = re.sub(neutralface + '|-[._]-', \"<neutralface>\", text)\n",
    "    text = re.sub(r'<3|&lt;3',\"<heart>\", text)\n",
    "    text = re.sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', \"<number>\", text)\n",
    "    text = re.sub(r',', ' ', text)\n",
    "    text = re.sub(r'#\\S+', lambda x: split_hashtag(x.group()), text) \n",
    "    text = re.sub(r'([!?.<>]){2,}', lambda x: x.group()[1] + \" <repeat> \", text)\n",
    "    text = re.sub(r'\\b(\\S*?)(.)\\2{3,}\\b', lambda x: ''.join(x.group()) + ' <elong> ', text)\n",
    "    text = re.sub(r'[A-Z]{2,}', lambda x: x.group().lower() + ' <allcaps> ', text)\n",
    "\n",
    "  text = text.lower()\n",
    "  return text\n",
    "\n",
    "def binarize(count):\n",
    "  return 1 if count > 0 else 0\n",
    "\n",
    "def read_write_dataset(datafile, write=False, twitter=True):\n",
    "  cleaned_data = datafile.split('.')[0] + '_cleaned.csv'\n",
    "\n",
    "  if write:\n",
    "    with open(datafile,'rb') as data_file:\n",
    "      data = pd.read_csv( data_file, header = 0, index_col = 0, quoting = 0 )\n",
    "      data['tweet'] = data.apply(lambda row: clean(row['tweet'], twitter), axis=1)\n",
    "      data['hate_speech'] = data.apply(lambda row: binarize(row['hate_speech']), axis=1)\n",
    "      data['offensive_language'] = data.apply(lambda row: binarize(row['offensive_language']), axis=1)\n",
    "      data['neither'] = data.apply(lambda row: binarize(row['neither']), axis=1)\n",
    "\n",
    "    with open(cleaned_data,'wb') as output_file:\n",
    "      data.to_csv( output_file, header = True, quoting = 0, columns=['tweet', 'hate_speech', 'offensive_language', 'neither', 'class'] )\n",
    "  \n",
    "  else:\n",
    "    with open(cleaned_data,'rb') as data_file:\n",
    "      data = pd.read_csv( data_file, header = 0, quoting = 0, \n",
    "        dtype = {'hate_speech': np.int32, 'offensive_language': np.int32, 'neither': np.int32, 'class': np.int32} )\n",
    "\n",
    "  return data\n",
    "\n",
    "def save_files(prefix, tier, indices, data):\n",
    "  tier_data = data.ix[indices] if indices else data\n",
    "\n",
    "  with open(os.path.join(prefix, tier + '.x'), 'wb') as x_file:\n",
    "    tier_data.to_csv( x_file, header = True, quoting = 0, columns=['tweet'] )\n",
    "\n",
    "  with open(os.path.join(prefix, tier + '.y'), 'wb') as y_file:\n",
    "    tier_data.to_csv( y_file, header = True, quoting = 0, columns=['hate_speech', 'offensive_language', 'neither', 'class'] )\n",
    "\n",
    "def split_tier(data_prefix, data, train_percentage = 0.8, shuffle=False):\n",
    "  train_i, test_i = train_test_split( data.index , train_size = train_percentage, random_state = 44 )\n",
    "  save_files(data_prefix, 'train', train_i, data)\n",
    "  save_files(data_prefix, 'test', test_i, data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  args = setup_args()\n",
    "  data = read_write_dataset(args.rewrite, args.datafile, args.twitter)\n",
    "\n",
    "  print(\"Splitting the dataset into train and validation\")\n",
    "  data_prefix = os.path.join(\"data\", \"twitter_davidson\")\n",
    "  # split_tier(data_prefix, data, 0.8)\n",
    "  save_files(data_prefix, 'all', None, data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
