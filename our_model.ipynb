{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davidson Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = tweet.split() #[stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bitch',\n",
       " 'ghetto',\n",
       " 'ratchett',\n",
       " 'illegal alien',\n",
       " 'no homo',\n",
       " 'retarded',\n",
       " 'lame',\n",
       " 'crazy',\n",
       " 'dumb',\n",
       " \"that's so gay\",\n",
       " 'whore',\n",
       " 'ho',\n",
       " 'slut',\n",
       " \"Bisexuality doesn't really exist. People are just gay or straight.\",\n",
       " 'i think everyone is bisexual',\n",
       " \"You're too femme to be bisexual\",\n",
       " \"You're too butch to be bisexual\",\n",
       " 'Bisexual people just want straight privilege',\n",
       " 'Bisexual people are just greedy and want to have sex with everyone.',\n",
       " 'Who do you see yourself ending up with?',\n",
       " 'Tranny',\n",
       " \"That person doesn't really look like a woman\",\n",
       " \"That person doesn't really look like a man\",\n",
       " 'What is your REAL name? I mean the one you were given at birth',\n",
       " 'He-She',\n",
       " 'What are you REALLY? Have you had surgery?',\n",
       " 'cunt',\n",
       " 'twat',\n",
       " 'pussy',\n",
       " 'thot',\n",
       " 'ugly',\n",
       " 'you guys',\n",
       " \"I'm being such a fat-ass\",\n",
       " \"I'm being so fat right now!\"]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_that_hurt = {\n",
    "    'bitch': 'Targets and dehumanizes women, even if used toward men, including queer and gay men. Devalues women and femininity. Reinforces sexism.',\n",
    "    'ghetto' :'Describes something or someone as cheap, worn out, poor, dangerous, etc. Reference to housing communities that are impoverished and disproportionately impact people of color. Associates people of color with these negative characteristics.',\n",
    "    'ratchett':'Describes something or someone as cheap, worn out, poor, dangerous, etc. Reference to housing communities that are impoverished and disproportionately impact people of color. Associates people of color with these negative characteristics.',\n",
    "    'illegal alien': 'Reduces undocumented immigrants to something less than human. Fixates on legal status instead of people as individuals. Asserts that some people belong here more than others do. Ignores political, social, and economic factors that impact people of color.',\n",
    "    'no homo': 'Stresses the speaker\\'s heterosexuality, masculinity, and/or other traits to avoid being perceived as LGBTQIA. Goes to great lengths to avoid association with anything queer. Reinforces that to be LGBTQIA is bad.',\n",
    "    'retarded': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'lame': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'crazy':'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'dumb': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'that\\'s so gay': 'Stigmatizes gay and queer people. Uses their identities to describe something as undesirable and bad. Replaces negative adjectives with words related to LGBTQIA identities.',\n",
    "    'whore': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'ho': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'slut': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'Bisexuality doesn\\'t really exist. People are just gay or straight.': 'This denies the fluidity of sexuality and dismisses people\\'s experiences and definitions of self. People deserve the right to define their own identities any way they wish and have those definitions honored.',\n",
    "    'i think everyone is bisexual': 'While this is often meant to acknowledge the fluidity of sexuality, it dismisses the reality of people who identify as bisexual and erases their experiences. It also invalidates the self-identifications of non-bisexual people.',\n",
    "    'You\\'re too femme to be bisexual':'Gender presentation does not indicate sexual orientation. Bisexual people have a wide range of gender presentations.',\n",
    "    'You\\'re too butch to be bisexual':'Gender presentation does not indicate sexual orientation. Bisexual people have a wide range of gender presentations.',\n",
    "    'Bisexual people just want straight privilege':'Bisexual people experience discrimination within straight communities and lesbian/gay communities. They never fully experience straight privilege because they do not identify as straight. Often their identities are made invisible and denied.',\n",
    "    'Bisexual people are just greedy and want to have sex with everyone.':'This stereotypes bisexual people and assumes they are all promiscuous - and that this is a bad thing. It creates negative attitudes toward sex and works against creating a sex positive climate. It also demonstrates an underlying belief that bisexuality is only about behavior and is not a legitimate identity.',\n",
    "    'Who do you see yourself ending up with?':'This is another way of implying one has to \"end up\" gay or straight and ignores bisexuality as an identity versus a relationship status. It also assumes everyone desires to be in a long-term monogamous relationship.',\n",
    "    'Tranny':'Whether or not someone identifies as trans*, calling anyone \"tranny\" is extremely offensive. While some folks within the trans* community may choose to reclaim this word for themselves, it is not a word that is okay to use to label another person or use as a joke.',\n",
    "    'That person doesn\\'t really look like a woman':'What does it mean to look like a man or woman? There are no set criteria. It also should not be assumed that all Trans Men strive to fit within dominant ideas of masculinity or all Trans Women strive to fit within dominant ideas of femininity, or that all Trans* people want to look like men or women. Gender presentation is fluid and distinct from gender identity, and all forms of gender expression deserve affirmation.',\n",
    "    'That person doesn\\'t really look like a man':'What does it mean to look like a man or woman? There are no set criteria. It also should not be assumed that all Trans Men strive to fit within dominant ideas of masculinity or all Trans Women strive to fit within dominant ideas of femininity, or that all Trans* people want to look like men or women. Gender presentation is fluid and distinct from gender identity, and all forms of gender expression deserve affirmation.',\n",
    "    'What is your REAL name? I mean the one you were given at birth':'This implies that the person\\'s gender identity and chosen name are not \"real\" and perpetuates the idea of Trans people as deceptive. It removes agency and any right to make decisions for themselves, and is incredibly invalidating. It presumes a right to intimate information, disregards privacy, and places Trans lives on public display.',\n",
    "    'He-She':'This hyphenated term is demeaning and invalidates an individual\\'s identity and the pronouns that they use.',\n",
    "    'What are you REALLY? Have you had surgery?': 'Asking anyone personal questions about their bodies and/or surgeries is invasive and inappropriate. We don\\'t ask cisgender people about what is under their clothes; we shouldn\\'t ask Trans* people either.',\n",
    "    'cunt':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'twat':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'pussy':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'thot':'Word created to express womxn or people who are sexually promiscuous. There are speculations that the word comes from the KKK organization that referred to Black women who were forced into prostitution (i.e. Sarah Baartman: Hottentot).',\n",
    "    'ugly':'Word used to put down someone for the way they look, can be connected back to white supremacist, ableist, sizeist standards of beauty.',\n",
    "    'you guys':'Erases the identities of people who are in the room. Generalizing a group of people to be masculine.',\n",
    "    'I\\'m being such a fat-ass':'Demeans and devalues fatness/fat bodies, reinforces harmful assumptions that fat people are gluttonous and are fat because they have no restraint around food. Also implies that there is an acceptable amount of food to eat and anything more is disgusting, or that enjoying food too much is disgusting.',\n",
    "    'I\\'m being so fat right now!':'Demeans and devalues fatness/fat bodies, reinforces harmful assumptions that fat people are gluttonous and are fat because they have no restraint around food. Also implies that there is an acceptable amount of food to eat and anything more is disgusting, or that enjoying food too much is disgusting.'\n",
    "}\n",
    "\n",
    "hurtfulWords = list(words_that_hurt.keys())\n",
    "hurtfulWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Feature #6 1) ID tweets with female pronouns 2) Check if these words are in the tweet \n",
    "\n",
    "#these words are used disproportionately often against women\n",
    "#the behaviour they describe often goes unremarked in men.\n",
    "#source: http://sacraparental.com/2016/05/14/everyday-misogyny-122-subtly-sexist-words-women/\n",
    "#EVERYDAY MISOGYNY: 122 SUBTLY SEXIST WORDS ABOUT WOMEN (AND WHAT TO DO ABOUT THEM)\n",
    "female_and_nongender_Pronouns = set(['you','she','its','their','yours',\n",
    "                                    'her', 'it', 'they', 'them',\n",
    "                                    'yourself', 'herself', 'themselves',\n",
    "                                    'your','hers'])\n",
    "\n",
    "pronouns = {'I': ('personal', True, 'first'),\n",
    " 'me': ('personal', True, 'first'),\n",
    " 'we': ('personal', False, 'first'),\n",
    " 'us': ('personal', False, 'first'),\n",
    " 'you': ('personal', False, 'second'),\n",
    " 'she': ('personal', True, 'third'),\n",
    " 'he': ('personal', True, 'third'),\n",
    " 'her': ('possessive', True, 'third'),\n",
    " 'him': ('personal', True, 'third'),\n",
    " 'it': ('personal', True, 'third'),\n",
    " 'they': ('personal', False, 'third'),\n",
    " 'them': ('personal', False, 'third'),\n",
    " 'myself': ('reflexive', True, 'first'),\n",
    " 'ourselves': ('reflexive', False, 'first'),\n",
    " 'yourself': ('reflexive', True, 'second'),\n",
    " 'yourselves': ('reflexive', False, 'second'),\n",
    " 'himself': ('reflexive', True, 'third'),\n",
    " 'herself': ('reflexive', True, 'third'),\n",
    " 'itself': ('reflexive', True, 'third'),\n",
    " 'themselves': ('reflexive', False, 'third'),\n",
    " 'my': ('possessive', True, 'first'),\n",
    " 'your': ('possessive', False, 'second'),\n",
    " 'his': ('possessive', True, 'third'),\n",
    " 'hers': ('possessive', True, 'third'),\n",
    " 'its': ('possessive', True, 'third'),\n",
    " 'our': ('possessive', False, 'first'),\n",
    " 'their': ('possessive', False, 'third'),\n",
    " 'mine': ('possessive', True, 'first'),\n",
    " 'yours': ('possessive', False, 'second'),\n",
    " 'ours': ('possessive', False, 'first')}\n",
    "\n",
    "female_offensive = ['bossy', 'abrasive', 'ball-buster', 'aggressive', \n",
    "'shrill', 'bolshy', 'intense', 'stroppy', 'forward', \n",
    "'mannish', 'gossipy', 'Dramatic', 'Drama Queen', 'Catty', \n",
    "'Bitchy', 'Nag', 'Cold', 'Ice queen', 'Shrew', 'Humourless',\n",
    "'Man-hater', 'Banshee', 'Fishwife', 'Lippy', 'Ditzy', 'Feminazi', \n",
    "'militant feminist', 'Bridezilla', 'Diva', 'Prima donna', 'Blonde moment',\n",
    "'Feisty', 'Supermum','Working mother', 'Career woman', 'Yummy mummy', 'Little old lady', \n",
    "'WAHM', 'Slut', 'Trollop','Frigid','Easy','Tease','Loose','Man-eater','Cougar',\n",
    "'Asking for it','prude','the town bike', 'Mutton dressed as lamb','Slutty','Curvy','Mumsy',\n",
    "'Cheap','That dress is flattering','Frumpy','Let herself go','Faded beauty','Mousey',\n",
    " 'Plus-size','Clotheshorse','Brunette ','Ladylike','Bubbly','Vivacious','Flirty',\n",
    "'Sassy','Chatty','Demure','Modest','Emotional','Hysterical','Hormonal',\n",
    "'Menstrual ',' pre-menstrual ','Flaky','Moody','Over-sensitive',\n",
    "'Clucky','Neurotic','Irrational','Baby brain','Baby weight','Mummy blogger',\n",
    "'Female engineer','That’s good, for a girl','Like a girl','run like a girl', \n",
    "'throw like a girl','Mumpreneur','Spinster','Barren','She wears the pants','Housewife',\n",
    "'Houseproud','Soccer mom','Mistress','Kept woman','Incompetent cervix',\n",
    "'Failure to progress','Elderly primagravida','Irritable uterus','Tomboy',\n",
    "'Girly','a girly girl','Little lady','Jail-bait','Heart-breaker',\n",
    "'pretty little thing','Catfight','Mommy wars','Caring','Compassionate','Hard-working',\n",
    "'Conscientious','Dependable','Diligent','Dedicated','Tactful','Interpersonal','Warm',\n",
    "'Helpful','Maternal', 'Princess', 'Heart-breaker']\n",
    "#most tweeted to Megyn Kelly by Trump and trump supperters\n",
    "#https://www.vox.com/2016/1/27/10852876/donald-trump-supporters-sexist-tweets-megyn-kelly\n",
    "trump_suppporters_megynKelly = [\"ugly\", \"cheap\", 'bitch', 'whore', 'bimbo',\n",
    "                                'cunt', 'hooker', 'slut', 'skank']\n",
    "others = ['hoe', 'pussy', 'bitches', 'fatty', 'fatass', 'fat-ass']\n",
    "offsensive_words_toward_women = female_offensive + trump_suppporters_megynKelly + others + hurtfulWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_offensive_words = set()\n",
    "for word in offsensive_words_toward_women:\n",
    "    female_offensive_words.add(word.lower())\n",
    "#female_offensive_words\n",
    "\n",
    "def check_offensive_to_women(text):\n",
    "    #split tweet by white space and make lower case\n",
    "    li = set([word.lower() for word in text.split()]) \n",
    "    isFemale = female_and_nongender_Pronouns.intersection(li)\n",
    "    if len(isFemale) == 0:\n",
    "        return False\n",
    "    isOffensive = female_offensive_words.intersection(li)\n",
    "    if isOffensive:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "#checkOffensive = check_offensive_to_women(\"She is so bossy\")\n",
    "#checkOffensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = open('ethnic_groups.txt','r').read().split('\\n')\n",
    "\n",
    "#demonstrative adjectives and other words that can inidicate targeting of a specific group\n",
    "targets = ['all', 'every', 'you', 'those', 'these', 'any', 'each', 'no', 'that', 'this', ]\n",
    "modality = ['should', 'can', 'can\\'t', 'cannot', 'won\\'t', 'will', 'want']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If tweet contains a targeted statement referring to a certain group, i.e. \"all you Asians\" or \"every Mexican\"\n",
    "#also checks if a group word is followed by some sort of modal verb\n",
    "\n",
    "def contains_target(words):\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() in targets:\n",
    "            if words[i+1].lower() in groups:\n",
    "                return 1\n",
    "        if words[i].lower() in groups:\n",
    "            if words[i+1].lower() in modality:\n",
    "                return 1\n",
    "            \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    #sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    #avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    #Our features\n",
    "    targeted = contains_target(words)\n",
    "    immigrant_ref = 0\n",
    "    if words.find('immigrant') or words.find('immigrants'):\n",
    "        immigrant_ref = 1\n",
    "    #isOffensiveToWomen = check_offensive_to_women(tweet)\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    #FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    #FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "        \n",
    "    features = [num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms,\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet, targeted, immigrant_ref]#, isOffensiveToWomen]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"num_chars\", \"num_chars_total\", \"num_terms\", \"num_words\", \"num_unique_words\", \"num_hashtags\", \\\n",
    "                    \"num_mentions\", \"num_urls\", \"is_retweet\", \"targeted\", \"immigrant_ref\"]#, \"isOffensiveToWomen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from allennlp.commands.elmo import ElmoEmbedder\\nfrom nltk.tokenize.treebank import TreebankWordTokenizer'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ELMo\n",
    "'''from allennlp.commands.elmo import ElmoEmbedder\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "small_X = tweets#.head(100)\n",
    "elmo_train_toks = [tokenizer.tokenize(ex) for ex in small_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_elmo_train_layers = list(elmo.embed_sentences(elmo_train_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19746"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert_serving.client import BertClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = tweets\n",
    "all_y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bert_train, bert_train_toks = bc.encode(\n",
    "#     list(all_X), show_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_reduce_mean(X):\n",
    "#     return X.mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bert_train_mean = bert_reduce_mean(X_bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "#M = np.concatenate([tfidf,pos,feats,X_elmo_train_layers],axis=1)\n",
    "\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "        [('select', SelectFromModel(LogisticRegression(class_weight='balanced',\n",
    "                                                  penalty=\"l1\", C=0.01))),\n",
    "        ('model', LogisticRegression(class_weight='balanced',penalty='l2'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{}] # Optionally add parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, \n",
    "                           param_grid,\n",
    "                           cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   6.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   7.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   7.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   34.3s finished\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.43      0.35       104\n",
      "           1       0.92      0.86      0.89      1507\n",
      "           2       0.68      0.75      0.71       364\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1975\n",
      "   macro avg       0.63      0.68      0.65      1975\n",
      "weighted avg       0.84      0.82      0.83      1975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report( y_test, y_preds )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without isOffensiveToWomen\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.29      0.43      0.35       104\n",
    "#           1       0.92      0.86      0.89      1507\n",
    "#           2       0.68      0.75      0.71       364\n",
    "\n",
    "#   micro avg       0.82      0.82      0.82      1975\n",
    "#   macro avg       0.63      0.68      0.65      1975\n",
    "#weighted avg       0.84      0.82      0.83      1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with is offensiveToWomen\n",
    "#             precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.28      0.42      0.34       104\n",
    "#           1       0.92      0.86      0.89      1507\n",
    "#           2       0.67      0.76      0.71       364\n",
    "\n",
    "#   micro avg       0.82      0.82      0.82      1975\n",
    "#   macro avg       0.62      0.68      0.64      1975\n",
    "#weighted avg       0.84      0.82      0.83      1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = df[['tweet', 'class']]\n",
    "misses = np.where(np.asarray(y_test) != y_preds)\n",
    "missed_preds = []\n",
    "for i in range(len(y_test)):\n",
    "    if np.asarray(y_test)[i] != y_preds[i]:\n",
    "        missed_preds.append(y_preds[i])\n",
    "    \n",
    "\n",
    "missed = [list(y_test.index)[i] for i in misses[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_tweets = all_tweets.iloc[missed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>@TonyO97 fuck i look like shopping at that tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>RT @DessantiGina: @TonyJRodriguez @WolfVanHale...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>Damn some Oreos would be so fucking clutch rig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>Dis nicca lame</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>@DivaMonRoe2uHoE @CheefPolo hoe hoe hoe, merry...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>Gonna straight hip check the next hillbilly wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19415</th>\n",
       "      <td>They're calling it #Sandy because the wind is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>I really just want to kill some towel head ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>@operationSAFE @GaltsGirl lived there and can ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>do they even make dresses any more that have s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>Good even-ink, honkies!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>RT @WORIDSTARHlPHOP: this kid definitely got t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>@Cee_Murda94 it's almost ya fucking birthday. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14040</th>\n",
       "      <td>&amp;#8220;@AustinMahone: just got a new fish it's...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>@kieffer_jason keep talking I'm going to make ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11152</th>\n",
       "      <td>Young Gem &amp;amp; Don Chief been killing it in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14629</th>\n",
       "      <td>@Ants_SNEweather Well you got to see pom-pom Pete</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>This cowboys an redskins game was a hell of a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>How long are they going to let the fools in #F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>Well now that I'm basically gone full retard, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>@yoKBFILTHY shit is trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18849</th>\n",
       "      <td>guess rubes doesn't want to go on a date with ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>RT @elchavaloko: I can see moose rockin himsel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>RT @__thaRealist: @Dono_44 yea that hoe was ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17348</th>\n",
       "      <td>@joeylattime @Mawson38 @ameriC00N thats just m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>#creamteam leaving hoes w #cockbreath</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13270</th>\n",
       "      <td>@charloosss @keepitplur @nicoleariel_ I'll chu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>Told my dad to go buy cookies for the graduati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>I just can't date a ghetto ass girl bruh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10904</th>\n",
       "      <td>Touchdown hoe ... touchdown hoe!!!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>Let's just say the hotel I've been staying at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>RT @KidnapYoGranny: if u dont want your heart ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>RT @exjon: Before covering up #IRS misdeeds, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>in real life and online I follow that. sorry f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6244</th>\n",
       "      <td>@Prophzilla before I went off to college I wan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10534</th>\n",
       "      <td>I'm sorry.. I'm sorry.. I can't fuc wit u no m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13302</th>\n",
       "      <td>Happy birthday to the biggest retard out there...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>#BlessJesus The #Crown of HIS head to the sole...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18516</th>\n",
       "      <td>RT @TheDouch3: Tweeting about ur boyfriend won...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>@penisgravy \\nIs dem? I wants to has my weenis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>RT @BadNewsAli: You really are a faggot if you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17103</th>\n",
       "      <td>RT @__bettyboo: You better pray I don't send y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>hoes pick me like dandelions #PickMe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16425</th>\n",
       "      <td>@Ray_Lakee #fag\\n Fuckin pirelli tire black as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>I'm a wigger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11719</th>\n",
       "      <td>@sassyharryballs ugly white bitch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15238</th>\n",
       "      <td>RT @TransaparenT: \"Hi mom, last night I was cu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>@triple6em96 @Hunglikerobby_ uh I think his mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16847</th>\n",
       "      <td>&amp;#8220;@Garricka_: Ima just block you hoes out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>@Huntermoore finger my pussy slowly with circu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12416</th>\n",
       "      <td>Take that and shove it up your ass, @KeithOlbe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>Fake niccas ain't far dog they right in ya face!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16615</th>\n",
       "      <td>@almuirSI Probably, yeah. By most accounts, he...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13142</th>\n",
       "      <td>You out there popping them pills fucking them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19191</th>\n",
       "      <td>I can tell when certain and specific people ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14037</th>\n",
       "      <td>@khamillkilroy &amp;#128514;&amp;#128514; Shy Glizzy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7588</th>\n",
       "      <td>RT @FoodPornsx: Deep Fried Oreos &amp;#128588;&amp;#12...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>RT @SSparklesDaily: The most beautiful women I...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>RT @_LilBaddiee_: I have a daughter to think a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>@InfidelAlie that should read suck on my bacon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  class\n",
       "8500   @TonyO97 fuck i look like shopping at that tra...      1\n",
       "2626   RT @DessantiGina: @TonyJRodriguez @WolfVanHale...      0\n",
       "15306  Damn some Oreos would be so fucking clutch rig...      1\n",
       "1739                                      Dis nicca lame      2\n",
       "6956   @DivaMonRoe2uHoE @CheefPolo hoe hoe hoe, merry...      2\n",
       "7172   Gonna straight hip check the next hillbilly wh...      1\n",
       "19415  They're calling it #Sandy because the wind is ...      1\n",
       "5116   I really just want to kill some towel head ter...      0\n",
       "10399  @operationSAFE @GaltsGirl lived there and can ...      2\n",
       "7983   do they even make dresses any more that have s...      1\n",
       "13326                            Good even-ink, honkies!      1\n",
       "9397   RT @WORIDSTARHlPHOP: this kid definitely got t...      1\n",
       "2189   @Cee_Murda94 it's almost ya fucking birthday. ...      1\n",
       "14040  &#8220;@AustinMahone: just got a new fish it's...      2\n",
       "10989  @kieffer_jason keep talking I'm going to make ...      1\n",
       "11152  Young Gem &amp; Don Chief been killing it in t...      1\n",
       "14629  @Ants_SNEweather Well you got to see pom-pom Pete      2\n",
       "4991   This cowboys an redskins game was a hell of a ...      2\n",
       "3018   How long are they going to let the fools in #F...      2\n",
       "13317  Well now that I'm basically gone full retard, ...      1\n",
       "4202                           @yoKBFILTHY shit is trash      1\n",
       "18849  guess rubes doesn't want to go on a date with ...      2\n",
       "1710   RT @elchavaloko: I can see moose rockin himsel...      1\n",
       "1580   RT @__thaRealist: @Dono_44 yea that hoe was ro...      1\n",
       "17348  @joeylattime @Mawson38 @ameriC00N thats just m...      0\n",
       "16171              #creamteam leaving hoes w #cockbreath      1\n",
       "13270  @charloosss @keepitplur @nicoleariel_ I'll chu...      2\n",
       "2740   Told my dad to go buy cookies for the graduati...      0\n",
       "4045            I just can't date a ghetto ass girl bruh      1\n",
       "10904               Touchdown hoe ... touchdown hoe!!!!!      1\n",
       "...                                                  ...    ...\n",
       "3410   Let's just say the hotel I've been staying at ...      2\n",
       "15535  RT @KidnapYoGranny: if u dont want your heart ...      0\n",
       "2498   RT @exjon: Before covering up #IRS misdeeds, c...      1\n",
       "3449   in real life and online I follow that. sorry f...      0\n",
       "6244   @Prophzilla before I went off to college I wan...      1\n",
       "10534  I'm sorry.. I'm sorry.. I can't fuc wit u no m...      1\n",
       "13302  Happy birthday to the biggest retard out there...      1\n",
       "5436   #BlessJesus The #Crown of HIS head to the sole...      2\n",
       "18516  RT @TheDouch3: Tweeting about ur boyfriend won...      1\n",
       "6161   @penisgravy \\nIs dem? I wants to has my weenis...      1\n",
       "12325  RT @BadNewsAli: You really are a faggot if you...      1\n",
       "17103  RT @__bettyboo: You better pray I don't send y...      0\n",
       "14625               hoes pick me like dandelions #PickMe      1\n",
       "16425  @Ray_Lakee #fag\\n Fuckin pirelli tire black as...      1\n",
       "3106                                        I'm a wigger      1\n",
       "11719                  @sassyharryballs ugly white bitch      0\n",
       "15238  RT @TransaparenT: \"Hi mom, last night I was cu...      1\n",
       "7562   @triple6em96 @Hunglikerobby_ uh I think his mo...      1\n",
       "16847  &#8220;@Garricka_: Ima just block you hoes out...      1\n",
       "2084   @Huntermoore finger my pussy slowly with circu...      1\n",
       "12416  Take that and shove it up your ass, @KeithOlbe...      1\n",
       "5140    Fake niccas ain't far dog they right in ya face!      1\n",
       "16615  @almuirSI Probably, yeah. By most accounts, he...      2\n",
       "13142  You out there popping them pills fucking them ...      1\n",
       "19191  I can tell when certain and specific people ar...      1\n",
       "14037       @khamillkilroy &#128514;&#128514; Shy Glizzy      2\n",
       "7588   RT @FoodPornsx: Deep Fried Oreos &#128588;&#12...      2\n",
       "3068   RT @SSparklesDaily: The most beautiful women I...      2\n",
       "2712   RT @_LilBaddiee_: I have a daughter to think a...      1\n",
       "6503   @InfidelAlie that should read suck on my bacon...      0\n",
       "\n",
       "[363 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90, 206)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tweets.loc[:,'prediction'] = missed_preds\n",
    "len(missed_tweets[(missed_tweets['class'] == 2)]), len(missed_tweets[(missed_tweets['class'] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with isOffensiveToWomen\n",
    "#(89, 214)\n",
    "\n",
    "#without isOffensiveToWomen\n",
    "#(90, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
