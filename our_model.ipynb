{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davidson Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = tweet.split() #[stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_slang_dict():\n",
    "    slang_dict = {}\n",
    "    with open(\"slang_to_words.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            slang_dict[tokens[1]] = tokens[0]\n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_slang_two_dict():\n",
    "    slang_dict_two = {}\n",
    "    with open(\"noslangdotcom.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split(':')\n",
    "            slang_dict[tokens[0]] = tokens[1]\n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_slang_three_dict():\n",
    "    slang_dict_three = {}\n",
    "    with open(\"internet_slangsDotNet.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('==')\n",
    "            slang_dict[tokens[0]] = tokens[1]\n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def load_slang_four():\n",
    "    slang_dict_four = {}\n",
    "    with open(\"common_twitter_abbreviations.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('=')\n",
    "            slang_dict[tokens[0]] = tokens[1]\n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(*dict_args):\n",
    "    \"\"\"\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result\n",
    "\n",
    "slang_dict = merge_dicts(slang_dict_one, slang_dict_two, slang_dict_three, slang_dict_four)\n",
    "#slang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace slang with definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def slang_sentiment(text):\n",
    "    text = remove_slang(text)\n",
    "    senti = get_sentiment_text(text)\n",
    "    return senti\n",
    "\n",
    "def positive(text, indicator):\n",
    "    text = remove_slang(text)\n",
    "    senti = get_sentiment_text(text)\n",
    "    return get_pos(text, indicator)\n",
    "\n",
    "def negative(text, indicator):\n",
    "    text = remove_slang(text)\n",
    "    senti = get_sentiment_text(text)\n",
    "    return get_pos(text, indicator)\n",
    "\n",
    "def objective(text, indicator):\n",
    "    text = remove_slang(text)\n",
    "    senti = get_sentiment_text(text)\n",
    "    return get_pos(text, indicator)\n",
    "    \n",
    "def remove_slang(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    soFar = ''\n",
    "    \n",
    "    for word in s: \n",
    "        if word.lower() in slang_dict:\n",
    "            soFar += slang_dict[word.lower()] + ' '\n",
    "        else:\n",
    "            soFar += word  + ' ' \n",
    "    return soFar.split(' ')\n",
    "\n",
    "def get_sentiment_text(strList):\n",
    "    #text = strList[:-1]\n",
    "    pos_values = nltk.pos_tag(text)\n",
    "    pos_senti = []\n",
    "    for (x, y) in pos_values:\n",
    "        if len(get_sentiment(x,y)) > 1:\n",
    "            pos_senti.append(get_sentiment(x,y))\n",
    "        else: \n",
    "            pos_senti.append([0, 0, 0])       \n",
    "    return pos_senti\n",
    "        \n",
    "def get_pos(text, indicator):\n",
    "    x = 0\n",
    "    pos = get_sentiment_text(text)\n",
    "    for v in pos:\n",
    "        x +=  v[indicator]\n",
    "    return x\n",
    "    \n",
    "#text2 = remove_slang(\"I'm so hungry right now\")\n",
    "#text = text2[:-1]\n",
    "#say = get_pos(text, 0)\n",
    "#say1 = get_pos(text, 1)\n",
    "#say2 = get_pos(text, 2)\n",
    "#print(say, say1, say2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "#Convert between the PennTreebank tags to simple Wordnet tags\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_sentiment(word,tag):\n",
    "#\"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "    \n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sentinment \n",
    "data = pd.read_csv('SentiWordNet_3.0.0.txt', sep='\\t', header=None)\n",
    "data.columns = [\"POS\",\"ID\",\"PosScore\",\"NegScore\",\"SynsetTerms\",\"Gloss\"]\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def contains_quotes(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word == '\"' or word == \"'\" else 0, s))\n",
    "    if score > 0: \n",
    "        return 1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if self-referential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "ethnic_groups = []\n",
    "with open('ethnic_groups_and_common_slurs.txt', 'r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        ethnic_groups.append((line.split('\\n'))[0].lower())\n",
    "\n",
    "\n",
    "#demonstrative adjectives and other words that can inidicate targeting of a specific group\n",
    "targets = ['all', 'every', 'you', 'those', 'these', 'any', 'each', 'no', 'that', 'this']\n",
    "modality = ['should', 'can', 'can\\'t', 'cannot', 'won\\'t', 'will', 'want', 'wants', 'are']\n",
    "reclaiming = ['proud', 'reclaim', 'reclaming', 'offensive', 'like']\n",
    "me = ['i\\'m', 'we', 'i', 'me', 'this']\n",
    "\n",
    "def contains_target_self_referential(text):\n",
    "    words = tknzr.tokenize(text)\n",
    "                \n",
    "    #check word in ethnic_groups comes before word in me   \n",
    "    #e.g. the beaner in me forgets I like beans\n",
    "    for word in ethnic_groups:\n",
    "        if word in words[0:]:\n",
    "            for key in me:\n",
    "                if key in words[words.index(word):]:\n",
    "                    return 1\n",
    "    \n",
    "    #check if word in me comes before word in reclaiming\n",
    "    #e.g. i'm a proud beaner\n",
    "    for key in me:\n",
    "        if key in words[0:]:\n",
    "            for word in reclaiming:\n",
    "                if word in words[words.index(key):]:\n",
    "                    return 1\n",
    "            #check if word in me comes before word in ethnic_groups\n",
    "            for word in ethnic_groups:\n",
    "                if word in words[words.index(key):]:\n",
    "                    return 1\n",
    "            #check if word in me comes before word in ethnic_groups\n",
    "            #e.g. We beaners have to stick together\n",
    "            for word in ethnic_groups:\n",
    "                if word in words[words.index(key):]:\n",
    "                    return 1\n",
    "    \n",
    "    \n",
    "    #check if word in reclaiming comes after modality \n",
    "    #e.g. all beaners should go home is offensive\n",
    "    for key in modality:\n",
    "        if key in words[0:]:\n",
    "            for word in reclaiming:\n",
    "                if word in words[words.index(key):]:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "#val = contains_target_self_referential(\"All beaners should go home is offensive\")\n",
    "#val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offensive to Women / Words that Hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_that_hurt = {\n",
    "    'bitch': 'Targets and dehumanizes women, even if used toward men, including queer and gay men. Devalues women and femininity. Reinforces sexism.',\n",
    "    'ghetto' :'Describes something or someone as cheap, worn out, poor, dangerous, etc. Reference to housing communities that are impoverished and disproportionately impact people of color. Associates people of color with these negative characteristics.',\n",
    "    'ratchett':'Describes something or someone as cheap, worn out, poor, dangerous, etc. Reference to housing communities that are impoverished and disproportionately impact people of color. Associates people of color with these negative characteristics.',\n",
    "    'illegal alien': 'Reduces undocumented immigrants to something less than human. Fixates on legal status instead of people as individuals. Asserts that some people belong here more than others do. Ignores political, social, and economic factors that impact people of color.',\n",
    "    'no homo': 'Stresses the speaker\\'s heterosexuality, masculinity, and/or other traits to avoid being perceived as LGBTQIA. Goes to great lengths to avoid association with anything queer. Reinforces that to be LGBTQIA is bad.',\n",
    "    'retarded': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'retard': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'lame': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'crazy':'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'dumb': 'Targets mental, emotional and physical disabilities as objects for ridicule. Used as synonyms for \"worthless,\" \"bad,\" \"unintelligent,\" \"incapable,\" etc.',\n",
    "    'that\\'s so gay': 'Stigmatizes gay and queer people. Uses their identities to describe something as undesirable and bad. Replaces negative adjectives with words related to LGBTQIA identities.',\n",
    "    'whore': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'ho': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'slut': 'Dismisses anyone seen as being \"too\" sexual, particularly sex workers, women, LGBTQI people and people of color. Perpetuates negativity toward sex itself. Regulates who is allowed to have it.',\n",
    "    'Bisexuality doesn\\'t really exist. People are just gay or straight.': 'This denies the fluidity of sexuality and dismisses people\\'s experiences and definitions of self. People deserve the right to define their own identities any way they wish and have those definitions honored.',\n",
    "    'i think everyone is bisexual': 'While this is often meant to acknowledge the fluidity of sexuality, it dismisses the reality of people who identify as bisexual and erases their experiences. It also invalidates the self-identifications of non-bisexual people.',\n",
    "    'You\\'re too femme to be bisexual':'Gender presentation does not indicate sexual orientation. Bisexual people have a wide range of gender presentations.',\n",
    "    'You\\'re too butch to be bisexual':'Gender presentation does not indicate sexual orientation. Bisexual people have a wide range of gender presentations.',\n",
    "    'Bisexual people just want straight privilege':'Bisexual people experience discrimination within straight communities and lesbian/gay communities. They never fully experience straight privilege because they do not identify as straight. Often their identities are made invisible and denied.',\n",
    "    'Bisexual people are just greedy and want to have sex with everyone.':'This stereotypes bisexual people and assumes they are all promiscuous - and that this is a bad thing. It creates negative attitudes toward sex and works against creating a sex positive climate. It also demonstrates an underlying belief that bisexuality is only about behavior and is not a legitimate identity.',\n",
    "    'Who do you see yourself ending up with?':'This is another way of implying one has to \"end up\" gay or straight and ignores bisexuality as an identity versus a relationship status. It also assumes everyone desires to be in a long-term monogamous relationship.',\n",
    "    'Tranny':'Whether or not someone identifies as trans*, calling anyone \"tranny\" is extremely offensive. While some folks within the trans* community may choose to reclaim this word for themselves, it is not a word that is okay to use to label another person or use as a joke.',\n",
    "    'That person doesn\\'t really look like a woman':'What does it mean to look like a man or woman? There are no set criteria. It also should not be assumed that all Trans Men strive to fit within dominant ideas of masculinity or all Trans Women strive to fit within dominant ideas of femininity, or that all Trans* people want to look like men or women. Gender presentation is fluid and distinct from gender identity, and all forms of gender expression deserve affirmation.',\n",
    "    'That person doesn\\'t really look like a man':'What does it mean to look like a man or woman? There are no set criteria. It also should not be assumed that all Trans Men strive to fit within dominant ideas of masculinity or all Trans Women strive to fit within dominant ideas of femininity, or that all Trans* people want to look like men or women. Gender presentation is fluid and distinct from gender identity, and all forms of gender expression deserve affirmation.',\n",
    "    'What is your REAL name? I mean the one you were given at birth':'This implies that the person\\'s gender identity and chosen name are not \"real\" and perpetuates the idea of Trans people as deceptive. It removes agency and any right to make decisions for themselves, and is incredibly invalidating. It presumes a right to intimate information, disregards privacy, and places Trans lives on public display.',\n",
    "    'He-She':'This hyphenated term is demeaning and invalidates an individual\\'s identity and the pronouns that they use.',\n",
    "    'What are you REALLY? Have you had surgery?': 'Asking anyone personal questions about their bodies and/or surgeries is invasive and inappropriate. We don\\'t ask cisgender people about what is under their clothes; we shouldn\\'t ask Trans* people either.',\n",
    "    'cunt':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'twat':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'pussy':'Using words that refer to people with vaginas to express that someone is weak or emotional. Dehumanizes womxn and perpetuates misogyny and sexism.',\n",
    "    'thot':'Word created to express womxn or people who are sexually promiscuous. There are speculations that the word comes from the KKK organization that referred to Black women who were forced into prostitution (i.e. Sarah Baartman: Hottentot).',\n",
    "    'ugly':'Word used to put down someone for the way they look, can be connected back to white supremacist, ableist, sizeist standards of beauty.',\n",
    "    'you guys':'Erases the identities of people who are in the room. Generalizing a group of people to be masculine.',\n",
    "    'I\\'m being such a fat-ass':'Demeans and devalues fatness/fat bodies, reinforces harmful assumptions that fat people are gluttonous and are fat because they have no restraint around food. Also implies that there is an acceptable amount of food to eat and anything more is disgusting, or that enjoying food too much is disgusting.',\n",
    "    'I\\'m being so fat right now!':'Demeans and devalues fatness/fat bodies, reinforces harmful assumptions that fat people are gluttonous and are fat because they have no restraint around food. Also implies that there is an acceptable amount of food to eat and anything more is disgusting, or that enjoying food too much is disgusting.'\n",
    "}\n",
    "\n",
    "hurtfulWords = list(words_that_hurt.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Feature #6 1) ID tweets with female pronouns 2) Check if these words are in the tweet \n",
    "\n",
    "#these words are used disproportionately often against women\n",
    "#the behaviour they describe often goes unremarked in men.\n",
    "#source: http://sacraparental.com/2016/05/14/everyday-misogyny-122-subtly-sexist-words-women/\n",
    "#EVERYDAY MISOGYNY: 122 SUBTLY SEXIST WORDS ABOUT WOMEN (AND WHAT TO DO ABOUT THEM)\n",
    "female_and_nongender_Pronouns = set(['you','she','its','their','yours',\n",
    "                                    'her', 'it', 'they', 'them',\n",
    "                                    'yourself', 'herself', 'themselves',\n",
    "                                    'your','hers'])\n",
    "\n",
    "pronouns = {'I': ('personal', True, 'first'),\n",
    " 'me': ('personal', True, 'first'),\n",
    " 'we': ('personal', False, 'first'),\n",
    " 'us': ('personal', False, 'first'),\n",
    " 'you': ('personal', False, 'second'),\n",
    " 'she': ('personal', True, 'third'),\n",
    " 'he': ('personal', True, 'third'),\n",
    " 'her': ('possessive', True, 'third'),\n",
    " 'him': ('personal', True, 'third'),\n",
    " 'it': ('personal', True, 'third'),\n",
    " 'they': ('personal', False, 'third'),\n",
    " 'them': ('personal', False, 'third'),\n",
    " 'myself': ('reflexive', True, 'first'),\n",
    " 'ourselves': ('reflexive', False, 'first'),\n",
    " 'yourself': ('reflexive', True, 'second'),\n",
    " 'yourselves': ('reflexive', False, 'second'),\n",
    " 'himself': ('reflexive', True, 'third'),\n",
    " 'herself': ('reflexive', True, 'third'),\n",
    " 'itself': ('reflexive', True, 'third'),\n",
    " 'themselves': ('reflexive', False, 'third'),\n",
    " 'my': ('possessive', True, 'first'),\n",
    " 'your': ('possessive', False, 'second'),\n",
    " 'his': ('possessive', True, 'third'),\n",
    " 'hers': ('possessive', True, 'third'),\n",
    " 'its': ('possessive', True, 'third'),\n",
    " 'our': ('possessive', False, 'first'),\n",
    " 'their': ('possessive', False, 'third'),\n",
    " 'mine': ('possessive', True, 'first'),\n",
    " 'yours': ('possessive', False, 'second'),\n",
    " 'ours': ('possessive', False, 'first')}\n",
    "\n",
    "female_offensive = ['bossy', 'abrasive', 'ball-buster', 'aggressive', \n",
    "'shrill', 'bolshy', 'intense', 'stroppy', 'forward', \n",
    "'mannish', 'gossipy', 'Dramatic', 'Drama Queen', 'Catty', \n",
    "'Bitchy', 'Nag', 'Cold', 'Ice queen', 'Shrew', 'Humourless',\n",
    "'Man-hater', 'Banshee', 'Fishwife', 'Lippy', 'Ditzy', 'Feminazi', \n",
    "'militant feminist', 'Bridezilla', 'Diva', 'Prima donna', 'Blonde moment',\n",
    "'Feisty', 'Supermum','Working mother', 'Career woman', 'Yummy mummy', 'Little old lady', \n",
    "'WAHM', 'Slut', 'Trollop','Frigid','Easy','Tease','Loose','Man-eater','Cougar',\n",
    "'Asking for it','prude','the town bike', 'Mutton dressed as lamb','Slutty','Curvy','Mumsy',\n",
    "'Cheap','That dress is flattering','Frumpy','Let herself go','Faded beauty','Mousey',\n",
    " 'Plus-size','Clotheshorse','Brunette ','Ladylike','Bubbly','Vivacious','Flirty',\n",
    "'Sassy','Chatty','Demure','Modest','Emotional','Hysterical','Hormonal',\n",
    "'Menstrual ',' pre-menstrual ','Flaky','Moody','Over-sensitive',\n",
    "'Clucky','Neurotic','Irrational','Baby brain','Baby weight','Mummy blogger',\n",
    "'Female engineer','Thatâ€™s good, for a girl','Like a girl','run like a girl', \n",
    "'throw like a girl','Mumpreneur','Spinster','Barren','She wears the pants','Housewife',\n",
    "'Houseproud','Soccer mom','Mistress','Kept woman','Incompetent cervix',\n",
    "'Failure to progress','Elderly primagravida','Irritable uterus','Tomboy',\n",
    "'Girly','a girly girl','Little lady','Jail-bait','Heart-breaker',\n",
    "'pretty little thing','Catfight','Mommy wars','Caring','Compassionate','Hard-working',\n",
    "'Conscientious','Dependable','Diligent','Dedicated','Tactful','Interpersonal','Warm',\n",
    "'Helpful','Maternal', 'Princess', 'Heart-breaker']\n",
    "#most tweeted to Megyn Kelly by Trump and trump supperters\n",
    "#https://www.vox.com/2016/1/27/10852876/donald-trump-supporters-sexist-tweets-megyn-kelly\n",
    "trump_suppporters_megynKelly = [\"ugly\", \"cheap\", 'bitch', 'whore', 'bimbo',\n",
    "                                'cunt', 'hooker', 'slut', 'skank']\n",
    "others = ['hoe', 'pussy', 'bitches', 'fatty', 'fatass', 'fat-ass']\n",
    "offsensive_words_toward_women = female_offensive + trump_suppporters_megynKelly + others + hurtfulWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_offensive_words = set()\n",
    "for word in offsensive_words_toward_women:\n",
    "    female_offensive_words.add(word.lower())\n",
    "#female_offensive_words\n",
    "\n",
    "def check_offensive_to_women(text):\n",
    "    #split tweet by white space and make lower case\n",
    "    li = set([word.lower() for word in text.split()]) \n",
    "    isFemale = female_and_nongender_Pronouns.intersection(li)\n",
    "    if len(isFemale) == 0:\n",
    "        return 0\n",
    "    isOffensive = female_offensive_words.intersection(li)\n",
    "    if isOffensive:\n",
    "        return len(isOffensive)\n",
    "    return 0\n",
    "    \n",
    "#checkOffensive = check_offensive_to_women(\"She is so hoe bossy\")\n",
    "#checkOffensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nrc_emotions_df = pd.read_csv(\"nrc_emotions.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC emotions \n",
    "Read in nrc_emotions_df as list values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = nrc_emotions_df.loc[nrc_emotions_df['anger']][['term']].values\n",
    "anticipation = nrc_emotions_df.loc[nrc_emotions_df['anticipation']][['term']].values\n",
    "disgust = nrc_emotions_df.loc[nrc_emotions_df['disgust']][['term']].values\n",
    "fear = nrc_emotions_df.loc[nrc_emotions_df['fear']][['term']].values\n",
    "joy = nrc_emotions_df.loc[nrc_emotions_df['joy']][['term']].values\n",
    "sadness = nrc_emotions_df.loc[nrc_emotions_df['sadness']][['term']].values\n",
    "surprise = nrc_emotions_df.loc[nrc_emotions_df['surprise']][['term']].values\n",
    "trust = nrc_emotions_df.loc[nrc_emotions_df['trust']][['term']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def anger_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in anger else 0, s))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def anticipation_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in anticipation else 0, s))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def disgust_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in disgust else 0, s))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def joy_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in joy else 0, s))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def fear_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in fear else 0, s))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def sadness_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in sadness else 0, s))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def surprise_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in surprise else 0, s))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def trust_count(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    score = sum(map(lambda word : 1 if word in trust else 0, s))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    #sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    text_only = preprocess(tweet) #Get text only\n",
    "    words = remove_slang(text_only) #replace slang/abbreviations with full words \n",
    "    \n",
    "    senti = slang_sentiment(text_only)\n",
    "    pos = positive(tweet, 0)\n",
    "    neg = negative(tweet, 1)\n",
    "    obj = objective(tweet, 2)\n",
    "    \n",
    "    no_slang_str = ''.join(words)\n",
    "    trustCount = trust_count(no_slang_str)\n",
    "    surpriseCount = surprise_count(no_slang_str)\n",
    "    sadnessCount = sadness_count(no_slang_str)\n",
    "    fearCount = fear_count(no_slang_str)\n",
    "    joyCount = joy_count(no_slang_str)\n",
    "    disgustCount = disgust_count(no_slang_str)\n",
    "    anticipationCount = anticipation_count(no_slang_str)\n",
    "    angerCount = anger_count(no_slang_str)\n",
    "    isSelfReferential = contains_target_self_referential(no_slang_str)\n",
    "    hasQuotes = contains_quotes(tweet)\n",
    "    \n",
    "    syllables = textstat.syllable_count(text_only)\n",
    "    num_chars = sum(len(w) for w in text_only)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(tweet.split())\n",
    "    #avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(text_only.split()))\n",
    "    #Our features\n",
    "    targeted = contains_target(text_only)\n",
    "    immigrant_ref = 0\n",
    "    if text_only.find('immigrant') or text_only.find('immigrants'):\n",
    "        immigrant_ref = 1\n",
    "    isOffensiveToWomen = check_offensive_to_women(tweet)\n",
    "    \n",
    "\n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    #FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    #FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "        \n",
    "    features = [num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms,\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet, targeted, immigrant_ref, isOffensiveToWomen,\n",
    "                trustCount, surpriseCount, sadnessCount, angerCount, fearCount, \n",
    "                joyCount, disgustCount, anticipationCount, isSelfReferential, hasQuotes, senti, pos, neg, obj]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below doesn't lower case the words in groups\n",
    "#groups = open('ethnic_groups_and_common_slurs.txt','r').read().split('\\n')\n",
    "\n",
    "#make all words lowercase\n",
    "ethnic_groups = []\n",
    "with open('ethnic_groups_and_common_slurs.txt', 'r') as fileinput:\n",
    "    for line in fileinput:\n",
    "        ethnic_groups.append((line.split('\\n'))[0].lower())\n",
    "#print(ethnic_groups)\n",
    "        \n",
    "#demonstrative adjectives and other words that can inidicate targeting of a specific group\n",
    "targets = ['all', 'every', 'you', 'those', 'these', 'any', 'each', 'no', 'that', 'this', ]\n",
    "modality = ['should', 'can', 'can\\'t', 'cannot', 'won\\'t', 'will', 'want']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If tweet contains a targeted statement referring to a certain group, i.e. \"all you Asians\" or \"every Mexican\"\n",
    "#also checks if a group word is followed by some sort of modal verb\n",
    "\n",
    "\n",
    "#NOTE ----- THIS DOESN\"T WORK THIS IS ITERATING OVER CHARS NOT WORDS --------\n",
    "'''def contains_target(words):\n",
    "    for i in range(len(words)):\n",
    "        print(words[i])\n",
    "        if words[i].lower() in targets:\n",
    "            print(word)\n",
    "            if words[i+1].lower() in groups:\n",
    "                return 1\n",
    "        if words[i].lower() in groups:\n",
    "            if words[i+1].lower() in modality:\n",
    "                return 1 \n",
    "    return 0\n",
    "val = contains_target(\"all you Asians\")\n",
    "val'''\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def contains_target(text):\n",
    "    s = tknzr.tokenize(text)\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        if s[i].lower() in targets:\n",
    "            if i != len(s)-1:\n",
    "                if s[i+1].lower() in ethnic_groups:\n",
    "                    return 1\n",
    "            \n",
    "        elif s[i].lower() in ethnic_groups:\n",
    "            if i != len(s)-1:\n",
    "                if s[i+1].lower() in modality:\n",
    "                    return 1            \n",
    "        return 0\n",
    "\n",
    "text = \"beaners\"\n",
    "val = contains_target(text)\n",
    "val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"num_chars\", \"num_chars_total\", \"num_terms\", \"num_words\", \"num_unique_words\", \"num_hashtags\", \\\n",
    "                    \"num_mentions\", \"num_urls\", \"is_retweet\", \"targeted\", \"immigrant_ref\", \"isOffensiveToWomen\",\n",
    "                \"trustCount\", \"surpriseCount\", \"sadnessCount\", \"angerCount\", \"fearCount\", \n",
    "              \"joyCount\", \"disgustCount\", \"anticipationCount\", \"isSelfReferential\", \"hasQuotes\", \"senti\", \"pos\", \"neg\", \"obj\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19746"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN FROM HERE WITHOUT ELMO AND BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = tweets\n",
    "all_y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "#M = np.concatenate([tfidf,pos,feats,X_elmo_train_layers],axis=1)\n",
    "\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "        [('select', SelectFromModel(LogisticRegression(class_weight='balanced',\n",
    "                                                  penalty=\"l1\", C=0.01))),\n",
    "        ('model', LogisticRegression(class_weight='balanced',penalty='l2'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{}] # Optionally add parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, \n",
    "                           param_grid,\n",
    "                           cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   7.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   5.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   30.5s finished\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33       104\n",
      "           1       0.92      0.86      0.89      1507\n",
      "           2       0.68      0.75      0.71       364\n",
      "\n",
      "    accuracy                           0.82      1975\n",
      "   macro avg       0.62      0.68      0.64      1975\n",
      "weighted avg       0.84      0.82      0.83      1975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report( y_test, y_preds )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.34       104\n",
      "           1       0.92      0.86      0.89      1507\n",
      "           2       0.68      0.76      0.72       364\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1975\n",
      "   macro avg       0.63      0.68      0.65      1975\n",
      "weighted avg       0.84      0.82      0.83      1975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#before we added features\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without isOffensiveToWomen\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.29      0.43      0.35       104\n",
    "#           1       0.92      0.86      0.89      1507\n",
    "#           2       0.68      0.75      0.71       364\n",
    "\n",
    "#   micro avg       0.82      0.82      0.82      1975\n",
    "#   macro avg       0.63      0.68      0.65      1975\n",
    "#weighted avg       0.84      0.82      0.83      1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With binary isOffensive\n",
    "#with is offensiveToWomen\n",
    "#             precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.28      0.42      0.34       104\n",
    "#           1       0.92      0.86      0.89      1507\n",
    "#           2       0.67      0.76      0.71       364\n",
    "\n",
    "#   micro avg       0.82      0.82      0.82      1975\n",
    "#   macro avg       0.62      0.68      0.64      1975\n",
    "#weighted avg       0.84      0.82      0.83      1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = df[['tweet', 'class']]\n",
    "misses = np.where(np.asarray(y_test) != y_preds)\n",
    "missed_preds = []\n",
    "for i in range(len(y_test)):\n",
    "    if np.asarray(y_test)[i] != y_preds[i]:\n",
    "        missed_preds.append(y_preds[i])\n",
    "    \n",
    "\n",
    "missed = [list(y_test.index)[i] for i in misses[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_tweets = all_tweets.iloc[missed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>@TonyO97 fuck i look like shopping at that tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>RT @DessantiGina: @TonyJRodriguez @WolfVanHale...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>Dis nicca lame</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>@DivaMonRoe2uHoE @CheefPolo hoe hoe hoe, merry...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>Gonna straight hip check the next hillbilly wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19415</th>\n",
       "      <td>They're calling it #Sandy because the wind is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>I really just want to kill some towel head ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>@operationSAFE @GaltsGirl lived there and can ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>do they even make dresses any more that have s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>Good even-ink, honkies!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15970</th>\n",
       "      <td>@Beautifulcallen nigger lol thts offensive ......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>RT @WORIDSTARHlPHOP: this kid definitely got t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>@Cee_Murda94 it's almost ya fucking birthday. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14040</th>\n",
       "      <td>&amp;#8220;@AustinMahone: just got a new fish it's...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>@kieffer_jason keep talking I'm going to make ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11152</th>\n",
       "      <td>Young Gem &amp;amp; Don Chief been killing it in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14629</th>\n",
       "      <td>@Ants_SNEweather Well you got to see pom-pom Pete</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>This cowboys an redskins game was a hell of a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>How long are they going to let the fools in #F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>@yoKBFILTHY shit is trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18849</th>\n",
       "      <td>guess rubes doesn't want to go on a date with ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>RT @elchavaloko: I can see moose rockin himsel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>RT @__thaRealist: @Dono_44 yea that hoe was ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17348</th>\n",
       "      <td>@joeylattime @Mawson38 @ameriC00N thats just m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>#creamteam leaving hoes w #cockbreath</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13270</th>\n",
       "      <td>@charloosss @keepitplur @nicoleariel_ I'll chu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>Told my dad to go buy cookies for the graduati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>I just can't date a ghetto ass girl bruh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10904</th>\n",
       "      <td>Touchdown hoe ... touchdown hoe!!!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10590</th>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>Let's just say the hotel I've been staying at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>RT @KidnapYoGranny: if u dont want your heart ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>RT @exjon: Before covering up #IRS misdeeds, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>in real life and online I follow that. sorry f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6244</th>\n",
       "      <td>@Prophzilla before I went off to college I wan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10534</th>\n",
       "      <td>I'm sorry.. I'm sorry.. I can't fuc wit u no m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13302</th>\n",
       "      <td>Happy birthday to the biggest retard out there...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>#BlessJesus The #Crown of HIS head to the sole...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>@penisgravy \\nIs dem? I wants to has my weenis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>RT @BadNewsAli: You really are a faggot if you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17103</th>\n",
       "      <td>RT @__bettyboo: You better pray I don't send y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>hoes pick me like dandelions #PickMe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16425</th>\n",
       "      <td>@Ray_Lakee #fag\\n Fuckin pirelli tire black as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14971</th>\n",
       "      <td>@Ms_Marrie I'm so tipsy... I just tried to rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>I'm a wigger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11719</th>\n",
       "      <td>@sassyharryballs ugly white bitch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>@triple6em96 @Hunglikerobby_ uh I think his mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16847</th>\n",
       "      <td>&amp;#8220;@Garricka_: Ima just block you hoes out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16603</th>\n",
       "      <td>Which expendable player can we put in with the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>@Huntermoore finger my pussy slowly with circu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12416</th>\n",
       "      <td>Take that and shove it up your ass, @KeithOlbe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>Fake niccas ain't far dog they right in ya face!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16615</th>\n",
       "      <td>@almuirSI Probably, yeah. By most accounts, he...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13142</th>\n",
       "      <td>You out there popping them pills fucking them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19191</th>\n",
       "      <td>I can tell when certain and specific people ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14037</th>\n",
       "      <td>@khamillkilroy &amp;#128514;&amp;#128514; Shy Glizzy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7588</th>\n",
       "      <td>RT @FoodPornsx: Deep Fried Oreos &amp;#128588;&amp;#12...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>RT @SSparklesDaily: The most beautiful women I...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>RT @_LilBaddiee_: I have a daughter to think a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>@InfidelAlie that should read suck on my bacon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  class\n",
       "8500   @TonyO97 fuck i look like shopping at that tra...      1\n",
       "2626   RT @DessantiGina: @TonyJRodriguez @WolfVanHale...      0\n",
       "1739                                      Dis nicca lame      2\n",
       "6956   @DivaMonRoe2uHoE @CheefPolo hoe hoe hoe, merry...      2\n",
       "7172   Gonna straight hip check the next hillbilly wh...      1\n",
       "19415  They're calling it #Sandy because the wind is ...      1\n",
       "5116   I really just want to kill some towel head ter...      0\n",
       "10399  @operationSAFE @GaltsGirl lived there and can ...      2\n",
       "7983   do they even make dresses any more that have s...      1\n",
       "13326                            Good even-ink, honkies!      1\n",
       "15970  @Beautifulcallen nigger lol thts offensive ......      1\n",
       "9397   RT @WORIDSTARHlPHOP: this kid definitely got t...      1\n",
       "2189   @Cee_Murda94 it's almost ya fucking birthday. ...      1\n",
       "14040  &#8220;@AustinMahone: just got a new fish it's...      2\n",
       "10989  @kieffer_jason keep talking I'm going to make ...      1\n",
       "11152  Young Gem &amp; Don Chief been killing it in t...      1\n",
       "14629  @Ants_SNEweather Well you got to see pom-pom Pete      2\n",
       "4991   This cowboys an redskins game was a hell of a ...      2\n",
       "3018   How long are they going to let the fools in #F...      2\n",
       "4202                           @yoKBFILTHY shit is trash      1\n",
       "18849  guess rubes doesn't want to go on a date with ...      2\n",
       "1710   RT @elchavaloko: I can see moose rockin himsel...      1\n",
       "1580   RT @__thaRealist: @Dono_44 yea that hoe was ro...      1\n",
       "17348  @joeylattime @Mawson38 @ameriC00N thats just m...      0\n",
       "16171              #creamteam leaving hoes w #cockbreath      1\n",
       "13270  @charloosss @keepitplur @nicoleariel_ I'll chu...      2\n",
       "2740   Told my dad to go buy cookies for the graduati...      0\n",
       "4045            I just can't date a ghetto ass girl bruh      1\n",
       "10904               Touchdown hoe ... touchdown hoe!!!!!      1\n",
       "10590  you's a muthaf***in lie &#8220;@LifeAsKing: @2...      1\n",
       "...                                                  ...    ...\n",
       "3410   Let's just say the hotel I've been staying at ...      2\n",
       "15535  RT @KidnapYoGranny: if u dont want your heart ...      0\n",
       "2498   RT @exjon: Before covering up #IRS misdeeds, c...      1\n",
       "3449   in real life and online I follow that. sorry f...      0\n",
       "6244   @Prophzilla before I went off to college I wan...      1\n",
       "10534  I'm sorry.. I'm sorry.. I can't fuc wit u no m...      1\n",
       "13302  Happy birthday to the biggest retard out there...      1\n",
       "5436   #BlessJesus The #Crown of HIS head to the sole...      2\n",
       "6161   @penisgravy \\nIs dem? I wants to has my weenis...      1\n",
       "12325  RT @BadNewsAli: You really are a faggot if you...      1\n",
       "17103  RT @__bettyboo: You better pray I don't send y...      0\n",
       "14625               hoes pick me like dandelions #PickMe      1\n",
       "16425  @Ray_Lakee #fag\\n Fuckin pirelli tire black as...      1\n",
       "14971  @Ms_Marrie I'm so tipsy... I just tried to rel...      1\n",
       "3106                                        I'm a wigger      1\n",
       "11719                  @sassyharryballs ugly white bitch      0\n",
       "7562   @triple6em96 @Hunglikerobby_ uh I think his mo...      1\n",
       "16847  &#8220;@Garricka_: Ima just block you hoes out...      1\n",
       "16603  Which expendable player can we put in with the...      2\n",
       "2084   @Huntermoore finger my pussy slowly with circu...      1\n",
       "12416  Take that and shove it up your ass, @KeithOlbe...      1\n",
       "5140    Fake niccas ain't far dog they right in ya face!      1\n",
       "16615  @almuirSI Probably, yeah. By most accounts, he...      2\n",
       "13142  You out there popping them pills fucking them ...      1\n",
       "19191  I can tell when certain and specific people ar...      1\n",
       "14037       @khamillkilroy &#128514;&#128514; Shy Glizzy      2\n",
       "7588   RT @FoodPornsx: Deep Fried Oreos &#128588;&#12...      2\n",
       "3068   RT @SSparklesDaily: The most beautiful women I...      2\n",
       "2712   RT @_LilBaddiee_: I have a daughter to think a...      1\n",
       "6503   @InfidelAlie that should read suck on my bacon...      0\n",
       "\n",
       "[361 rows x 2 columns]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92, 209, 60)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tweets.loc[:,'prediction'] = missed_preds\n",
    "len(missed_tweets[(missed_tweets['class'] == 2)]), len(missed_tweets[(missed_tweets['class'] == 1)]), len(missed_tweets[(missed_tweets['class'] == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with isOffensiveToWomen\n",
    "#(89, 214)\n",
    "\n",
    "#without isOffensiveToWomen\n",
    "#(90, 206)\n",
    "\n",
    "#(88, 205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
